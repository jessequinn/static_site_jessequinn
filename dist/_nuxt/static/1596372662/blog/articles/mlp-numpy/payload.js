__NUXT_JSONP__("/blog/articles/mlp-numpy", (function(a,b,c,d,e,f,g,h,i,j,k,l){return {data:[{article:{title:l,description:"I present some recent work in my study of multilayered perceptrons (MLPs)",img:"joao-tzanno-G9_Euqxpu4k-unsplash.jpg",alt:l,featured:0,author:{name:"Jesse Quinn",bio:"All about Jesse",img:"https:\u002F\u002Fimages.unsplash.com\u002Fphoto-1533636721434-0e2d61030955?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2550&q=80",alt:"profile photo"},publishedAt:"2019-10-28T03:00:00.000Z",updateAt:k,toc:[],body:{type:"root",children:[{type:b,tag:d,props:{},children:[{type:a,value:"I wanted to showcase some work I recently completed for a course in deep learning. I was recently challenged to make some MLPs with numpy and nothing else. Here is the "},{type:b,tag:"a",props:{href:"https:\u002F\u002Fgithub.com\u002Fjessequinn\u002FCoursera_DL\u002Fblob\u002Fmaster\u002Fweek2\u002FNumpyNN%20(honor).ipynb",rel:["nofollow","noopener","noreferrer"],target:"_blank"},children:[{type:a,value:e}]},{type:a,value:"."}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"First we are given several classes to work with that form the foundation to my MLPs."}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"class Layer:\n    \"\"\"\n    A building block. Each layer is capable of performing two things:\n    - Process input to get output:           output = layer.forward(input)\n        - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n        Some layers also have learnable parameters which they update during layer.backward.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n            # A dummy layer does nothing\n            pass\n\n    def forward(self, input):\n        \"\"\"\n        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n        \"\"\"\n        # A dummy layer just returns whatever it gets as input.\n        return input\n\n    def backward(self, input, grad_output):\n        \"\"\"\n        Performs a backpropagation step through the layer, with respect to the given input.\n        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n        d loss \u002F d x  = (d loss \u002F d layer) * (d layer \u002F d x)\n        Luckily, you already receive d loss \u002F d layer as input, so you only need to multiply it by d layer \u002F d x.\n        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss \u002F d layer\n        \"\"\"\n        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n        num_units = input.shape[1]\n        d_layer_d_input = np.eye(num_units)\n\n        return np.dot(grad_output, d_layer_d_input) # chain rule\n        \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We also work with the ReLU activation function although sigmoid, etc. could easily be implemented :"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"'''\nActivation function Rectified Linear Units (ReLU)\nf(x) = max(0, X)\nReLU function is a simple function which is zero for any \ninput value below zero and the same value for values greater than zero.\n'''\nclass ReLU(Layer):\n    def __init__(self):\n        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n        pass\n\n    def forward(self, input):\n        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n        return np.maximum(0,input)\n    \n    def backward(self, input, grad_output):\n        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n        relu_grad = input \u003E 0\n        return grad_output*relu_grad        \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"and finally the dense layer that is based on our original layer:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"class Dense(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        \"\"\"\n        A dense layer is a layer which performs a learned affine transformation:\n        f(x) = \u003CW*x\u003E + b\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.weights = np.random.randn(input_units, output_units)*0.01\n        self.biases = np.zeros(output_units)\n        self.grad_w = None\n\n    def forward(self,input):\n        \"\"\"\n        Perform an affine transformation:\n        f(x) = \u003CW*x\u003E + b\n\n        input shape: [batch, input_units]\n        output shape: [batch, output units]\n        \"\"\"\n        return np.dot(input,self.weights)+self.biases\n\n    def backward(self,input,grad_output):\n        grad_input = np.dot(grad_output,self.weights.T)       \n        grad_weights = input.T@grad_output\n        self.grad_w = grad_weights\n        grad_biases = grad_output.sum(axis=0)       \n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n\n        return grad_input\n        \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Some loss functions:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"def softmax_crossentropy_with_logits(logits,reference_answers):\n    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n\n    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n\n    return xentropy\n\ndef grad_softmax_crossentropy_with_logits(logits,reference_answers):\n    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n    ones_for_answers = np.zeros_like(logits)\n    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n\n    softmax = np.exp(logits) \u002F np.exp(logits).sum(axis=-1,keepdims=True)\n\n    return (- ones_for_answers + softmax) \u002F logits.shape[0]\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We are working with the MNist data set of images from 0 to 9 so we need to load that data in (in my case I use a special :"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom preprocessed_mnist import load_dataset\nX_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');\n    \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"based on the "},{type:b,tag:e,props:{},children:[{type:a,value:"preprocessed_mnist.py"}]},{type:a,value:" that contains:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"import keras\n\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) \u002F 255.\n    X_test = X_test.astype(float) \u002F 255.\n\n    # we reserve the last 10000 training examples for validation\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_val = X_val.reshape([X_val.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"We can now make a MLP:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"network = []\nnetwork.append(Dense(X_train.shape[1],100))\nnetwork.append(ReLU())\nnetwork.append(Dense(100,200))\nnetwork.append(ReLU())\nnetwork.append(Dense(200,10))\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"But first we need to define some useful functions:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"def forward(network, X):\n    \"\"\"\n    Compute activations of all network layers by applying them sequentially.\n    Return a list of activations for each layer. \n    Make sure last activation corresponds to network logits.\n    \"\"\"\n    activations = []\n    input = X\n\n    for layer in network:\n        activations.append(layer.forward(input))\n        input = activations[-1]\n\n    assert len(activations) == len(network)\n    return activations\n\ndef predict(network,X):\n    \"\"\"\n    Compute network predictions.\n    \"\"\"\n    logits = forward(network,X)[-1]\n    return logits.argmax(axis=-1)\n\ndef train(network,X,y):\n    \"\"\"\n    Train your network on a given batch of X and y.\n    You first need to run forward to get all layer activations.\n    Then you can run layer.backward going from last to first layer.\n\n    After you called backward for all layers, all Dense layers have already made one gradient step.\n    \"\"\"\n\n    # Get the layer activations\n    layer_activations = forward(network,X)\n    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n    logits = layer_activations[-1]\n\n    # Compute the loss and the initial gradient\n    loss = softmax_crossentropy_with_logits(logits,y)\n    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n\n    for i in range(len(network))[::-1]:\n        layer = network[i]\n        loss_grad = layer.backward(layer_inputs[i],loss_grad)\n\n    return np.mean(loss)\n    \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Our training loop:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert len(inputs) == len(targets)\n    if shuffle:\n        indices = np.random.permutation(len(inputs))\n    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n        if shuffle:\n                excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n                excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\n        \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Producing the graphs:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"from IPython.display import clear_output\ntrain_log = []\nval_log = []\nloss_log = []\n        \nfor epoch in range(25):\n    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n        loss_log.append(train(network,x_batch,y_batch))\n\n    train_log.append(np.mean(predict(network,X_train)==y_train))\n    val_log.append(np.mean(predict(network,X_val)==y_val))\n\n    clear_output()\n    print(\"Epoch\",epoch)\n    print(\"Train accuracy:\",train_log[-1])\n    print(\"Val accuracy:\",val_log[-1])\n    ax1 = plt.subplot(1,2,1)    \n    plt.plot(train_log,label='train accuracy')\n    plt.plot(val_log,label='val accuracy')\n    ax2 = plt.subplot(1,2,2)\n    plt.plot(loss_log,label='loss')\n    ax1.legend(loc='best')\n    ax2.legend(loc='best')\n    plt.grid()\n    plt.tight_layout()\n    plt.show()\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"Now onto some other things. Implemented the Xavier initialization to a new class:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"class DenseWithXavierInitialization(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1):\n        \"\"\"\n        A dense layer is a layer which performs a learned affine transformation:\n        f(x) = \u003CW*x\u003E + b\n        \"\"\"\n        self.learning_rate = learning_rate\n\n        # Xavier initialization\n        self.weights = np.random.randn(input_units, output_units)*np.sqrt(2\u002F(input_units+output_units))\n        self.biases = np.zeros(output_units)\n\n    def forward(self,input):\n        \"\"\"\n        Perform an affine transformation:\n        f(x) = \u003CW*x\u003E + b\n\n        input shape: [batch, input_units]\n        output shape: [batch, output units]\n        \"\"\"\n        return np.dot(input,self.weights)+self.biases\n\n    def backward(self,input,grad_output):\n        grad_input = np.dot(grad_output,self.weights.T)\n        grad_weights = input.T@grad_output\n        grad_biases = grad_output.sum(axis=0)\n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n\n        return grad_input\n        \n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"I also implemented L2 regularization:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"class DenseWithL2Regularization(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.1, L2_alpha=0):\n        \"\"\"\n        A dense layer is a layer which performs a learned affine transformation:\n        f(x) = \u003CW*x\u003E + b\n        \"\"\"\n        self.learning_rate = learning_rate\n\n        # initialize weights with small random numbers\n        self.weights = np.random.randn(input_units, output_units)*0.01\n        self.biases = np.zeros(output_units)\n        self.L2_alpha = L2_alpha\n    \n    def forward(self,input):\n        \"\"\"\n        Perform an affine transformation:\n        f(x) = \u003CW*x\u003E + b\n\n        input shape: [batch, input_units]\n        output shape: [batch, output units]\n        \"\"\"\n        return np.dot(input,self.weights)+self.biases\n\n    def backward(self,input,grad_output):\n        grad_input = np.dot(grad_output,self.weights.T)\n        grad_weights = input.T@grad_output + (2 * self.L2_alpha * self.weights)\n        grad_biases = grad_output.sum(axis=0)\n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        self.weights = self.weights - self.learning_rate * grad_weights\n        self.biases = self.biases - self.learning_rate * grad_biases\n\n        return grad_input\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"And finally I implemented RMSProp rather than SGD:"}]},{type:a,value:c},{type:b,tag:f,props:{className:[g]},children:[{type:b,tag:h,props:{className:[i,j]},children:[{type:b,tag:e,props:{},children:[{type:a,value:"class DenseWithRMSProp(Layer):\n    def __init__(self, input_units, output_units, learning_rate=0.0001, RMS_alpha=0.85):\n        \"\"\"\n        A dense layer is a layer which performs a learned affine transformation:\n        f(x) = \u003CW*x\u003E + b\n        \"\"\"\n        self.learning_rate = learning_rate\n\n        # initialize weights with small random numbers\n        self.weights = np.random.randn(input_units, output_units)*0.01\n        self.biases = np.zeros(output_units)\n        self.RMS_alpha = RMS_alpha\n        self.gW = 0.0\n        self.gB = 0.0\n        self.eps = 1e-8\n\n    def forward(self,input):\n        \"\"\"\n        Perform an affine transformation:\n        f(x) = \u003CW*x\u003E + b\n\n        input shape: [batch, input_units]\n        output shape: [batch, output units]\n        \"\"\"\n        return np.dot(input,self.weights)+self.biases\n\n    def backward(self,input,grad_output):\n        grad_input = np.dot(grad_output,self.weights.T)\n        grad_weights = input.T@grad_output\n        grad_biases = grad_output.sum(axis=0)\n        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n        self.gW = self.RMS_alpha*self.gW+(1-self.RMS_alpha)*(grad_weights**2)\n        self.gB = self.RMS_alpha*self.gB+(1-self.RMS_alpha)*(grad_biases**2)\n        self.weights = self.weights-self.learning_rate*(grad_weights\u002F(np.sqrt(self.gW)+self.eps))\n        self.biases = self.biases-self.learning_rate*(grad_biases\u002F(np.sqrt(self.gB)+self.eps))\n\n        return grad_input\n"}]}]}]},{type:a,value:c},{type:b,tag:d,props:{},children:[{type:a,value:"All in all it was fun and quite challenging to cover some of these basic ideas in deep learning just using Numpy."}]}]},dir:"\u002Farticles",path:"\u002Farticles\u002Fmlp-numpy",extension:".md",slug:"mlp-numpy",createdAt:"2020-07-22T00:27:09.706Z",updatedAt:"2020-07-22T00:30:14.073Z"},prev:{title:"Working with Linters, Fixers, Hooks for Github, etc.",updateAt:k,slug:"linters"},next:{title:"An interesting adventure with a PHP CMS \u002F\u002F Part 1",updateAt:k,slug:"php-cms-1"}}],fetch:[],mutations:[]}}("text","element","\n","p","code","div","nuxt-content-highlight","pre","language-text","line-numbers","2020-07-19T03:00:00.000Z","MLPs with Numpy")));